import os
import sys
import json
import requests
from dotenv import load_dotenv

# Add current and parent directories to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(current_dir)
sys.path.append(project_root)

# Load environment variables
load_dotenv()

# Import scraper and converter
from autoscout24_working_scraper import main as run_scraper
from scripts.convert_scraped_data import convert_all_data

import time

def push_to_backend(data_file):
    """Send converted data to Node.js backend one by one"""
    node_url = os.getenv("NODE_API_URL", "http://localhost:5000/ai/import-cars")
    print(f"\nğŸ“¡ Pushing data to: {node_url}")
    
    if not os.path.exists(data_file):
        print(f"âŒ Data file not found: {data_file}")
        return False

    with open(data_file, 'r', encoding='utf-8') as f:
        cars_data = json.load(f)

    print(f"ğŸ“¦ Total cars to sync: {len(cars_data)}")
    success_count = 0
    error_count = 0

    for i, car in enumerate(cars_data, 1):
        try:
            # Send one car at a time, disable individual notifications
            payload = {
                "cars": [car],
                "notify": False 
            } 
            response = requests.post(node_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                success_count += 1
                if i % 10 == 0 or i == len(cars_data):
                    print(f"  âœ… Progress: {i}/{len(cars_data)} synced...", end="\r")
            else:
                print(f"\nâŒ Error at car {i} ({car.get('title')}): {response.text}")
                error_count += 1
            
            # Small delay to prevent overwhelming the server
            time.sleep(0.1)

        except Exception as e:
            print(f"\nâŒ Connection error at car {i}: {e}")
            error_count += 1
            time.sleep(1) # Wait longer on error

    # 4. Trigger Summary Notification
    if success_count > 0:
        try:
            summary_url = node_url.replace("/import-cars", "/import-summary")
            requests.post(summary_url, json={"count": success_count}, timeout=10)
            print(f"\nğŸ”” Summary notification sent for {success_count} cars.")
        except:
            print("\nâš ï¸  Failed to send summary notification.")

    print(f"\n\nğŸ“Š Sync Results:")
    print(f"   - Success: {success_count}")
    print(f"   - Failed:  {error_count}")
    
    return success_count > 0

def run_automation():
    print("\nğŸš€ Starting Full Automation Cycle...")
    
    # 1. Run Scraper (Headless)
    try:
        print("\nStep 1: Running Scraper...")
        run_scraper(headless=True)
        print("âœ… Scraper Finished Successfully")
    except Exception as e:
        print(f"âŒ Scraper Failed: {e}")
        return

    # 2. Run Data Converter
    try:
        print("\nStep 2: Converting Data for API...")
        input_file = os.path.join(project_root, 'scrapers', 'output.json')
        output_file = os.path.join(project_root, 'data', 'raw', 'cars_data.json')
        convert_all_data(input_file, output_file)
        print("âœ… Data Conversion Finished Successfully")
    except Exception as e:
        print(f"âŒ Data Conversion Failed: {e}")
        return

    # 3. Push to Database via API
    try:
        print("\nStep 3: Syncing with Database...")
        output_file = os.path.join(project_root, 'data', 'raw', 'cars_data.json')
        success = push_to_backend(output_file)
        if success:
            print("âœ… Database Sync Finished Successfully")
        else:
            print("âš ï¸  Database Sync Failed (Check backend status)")
    except Exception as e:
        print(f"âŒ Database Sync Error: {e}")

    print("\nğŸ‰ Automation Cycle Completed Successfully!")

if __name__ == "__main__":
    run_automation()
